/**
 * E2E Test: Long Context Window Scenarios
 * 
 * Tests how the backend handles extremely long conversations:
 * 1. Multi-turn conversations exceeding 200k tokens
 * 2. Single responses that hit token output limits
 * 3. Context window management strategies
 * 
 * REQUIREMENTS:
 * - Backend server must be running
 * - SUPERMEMORY_API_KEY should be set (tests both with and without)
 * - These tests may take several minutes and use significant API quota
 */

import { describe, test, expect } from 'bun:test';
import { authenticateTestUser, loadGridSession } from '../setup/test-helpers';
import { supabase } from '../setup/supabase-test-client';

// Backend URL from environment or default
const BACKEND_URL = process.env.TEST_BACKEND_URL || 'http://localhost:3001';

// Helper: Generate a long message (for padding conversation history)
function generateLongMessage(topic: string, targetTokens: number): string {
  // Approximately 4 chars per token
  const targetChars = targetTokens * 4;
  const baseText = `This is a detailed explanation about ${topic}. `;
  const repetitions = Math.ceil(targetChars / baseText.length);
  return baseText.repeat(repetitions).substring(0, targetChars);
}

// Helper: Create conversation history with specific token count
function createLongConversationHistory(targetTokens: number): Array<{ role: 'user' | 'assistant', content: string }> {
  const messages: Array<{ role: 'user' | 'assistant', content: string }> = [];
  let currentTokens = 0;
  let turnNumber = 0;

  while (currentTokens < targetTokens) {
    turnNumber++;
    const remaining = targetTokens - currentTokens;
    const tokensPerMessage = Math.min(1000, remaining / 2); // 1k tokens per message

    // User message
    const userMessage = generateLongMessage(`topic ${turnNumber}`, tokensPerMessage);
    messages.push({ role: 'user', content: userMessage });
    currentTokens += tokensPerMessage;

    // Assistant message
    if (currentTokens < targetTokens) {
      const assistantMessage = generateLongMessage(`response to topic ${turnNumber}`, tokensPerMessage);
      messages.push({ role: 'assistant', content: assistantMessage });
      currentTokens += tokensPerMessage;
    }
  }

  return messages;
}

describe('Long Context Window Tests (E2E)', () => {
  test.skip('CRITICAL: should handle conversation exceeding 200k tokens', async () => {
    // TODO: Fix Anthropic API message format validation error
    // Error: messages.0.content.0.text.text: Field required
    // This happens when generating synthetic 200-message history
    // Needs investigation of how AI SDK convertToModelMessages handles large histories
    console.log('â­ï¸  Skipping 200k token test (needs format debugging)');
  }, 300000); // 5 minute timeout

  test.skip('CRITICAL: should handle very long single response (output token limit)', async () => {
    // TODO: This test genuinely times out because it requests a very long response
    // Needs either: 
    // 1. Longer timeout (5-10 min)
    // 2. Shorter request (but still tests output limits)
    // 3. Separate CI job with extended timeout
    console.log('â­ï¸  Skipping output token limit test (takes >3min)');
  }, 180000); // 3 minute timeout

  test('should verify context windowing fallback (no Supermemory)', async () => {
    console.log('âœ‚ï¸  Testing context windowing fallback...\n');
    console.log('   This test verifies manual windowing when Supermemory is unavailable');
    console.log();

    const { userId, accessToken } = await authenticateTestUser();
    const gridSession = await loadGridSession();

    const { data: conversation } = await supabase
      .from('conversations')
      .insert({
        user_id: userId,
        title: 'Test: Context Windowing',
      })
      .select()
      .single();

    const conversationId = conversation!.id;

    // Generate conversation exceeding windowing threshold (80k tokens)
    const TARGET_TOKENS = 100000; // Exceeds 80k threshold
    const conversationHistory = createLongConversationHistory(TARGET_TOKENS);

    console.log('ðŸ“Š Generated:', conversationHistory.length, 'messages');
    console.log('   Estimated:', TARGET_TOKENS.toLocaleString(), 'tokens');
    console.log('   Threshold: 80,000 tokens');
    console.log('   Backend should: Window to most recent messages');
    console.log();

    const requestBody = {
      messages: [
        ...conversationHistory.map(msg => ({
          ...msg,
          parts: [{ type: 'text', text: msg.content }]
        })),
        { 
          role: 'user', 
          content: 'Summarize our conversation.',
          parts: [{ type: 'text', text: 'Summarize our conversation.' }]
        },
      ],
      conversationId,
      gridSessionSecrets: gridSession.sessionSecrets,
      gridSession: {
        address: gridSession.address,
        authentication: gridSession.authentication,
      },
    };

    const response = await fetch(`${BACKEND_URL}/api/chat`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${accessToken}`,
      },
      body: JSON.stringify(requestBody),
    });

    expect(response.ok).toBe(true);

    // Just verify it completes without error
    let streamCompleted = false;

    if (response.body) {
      const reader = response.body.getReader();
      try {
        while (true) {
          const { done } = await reader.read();
          if (done) {
            streamCompleted = true;
            break;
          }
        }
      } finally {
        reader.releaseLock();
      }
    }

    expect(streamCompleted).toBe(true);
    console.log('âœ… Windowing fallback works correctly');
    console.log();

    // Cleanup
    await supabase.from('conversations').delete().eq('id', conversationId);

  }, 180000);
});

